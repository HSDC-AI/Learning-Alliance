"""
ä¿®æ”¹ç‰ˆçš„ AgentExecutor - é›†æˆyieldæµå¼å¤„ç†
å±•ç¤ºå¦‚ä½•åœ¨ç°æœ‰A2Aç³»ç»Ÿä¸­æ­£ç¡®ä½¿ç”¨yield
"""

import asyncio
import logging
import json

from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.utils import new_agent_text_message, new_data_artifact

from .streaming_agent import StreamingResourceProcessorOrchestrator

logger = logging.getLogger(__name__)


class StreamingOrchestratorAgentExecutor(AgentExecutor):
    """
    ä½¿ç”¨yieldçš„ç¼–æ’å™¨Agentæ‰§è¡Œå™¨
    è¿™æ˜¯åœ¨A2Aç³»ç»Ÿä¸­æ­£ç¡®ä½¿ç”¨yieldçš„æ–¹å¼
    """
    
    def __init__(self):
        # ä½¿ç”¨æµå¼å¤„ç†å™¨
        self.streaming_processor = StreamingResourceProcessorOrchestrator()
        logger.info("ğŸ­ StreamingOrchestratorAgentExecutor åˆå§‹åŒ–å®Œæˆ")
    
    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        """
        æ‰§è¡Œç¼–æ’ä»»åŠ¡ - ä½¿ç”¨yieldå®ç°æµå¼å“åº”
        è¿™æ˜¯Agentå†…éƒ¨æ­£ç¡®ä½¿ç”¨yieldçš„å…³é”®æ–¹æ³•
        """
        logger.info(f"ğŸ“¨ æ”¶åˆ°ç¼–æ’ä»»åŠ¡: {context.task_id}")
        
        try:
            # è§£æç”¨æˆ·æ¶ˆæ¯
            user_message = ""
            if context.request.params.message and context.request.params.message.parts:
                user_message = " ".join([
                    part.text for part in context.request.params.message.parts 
                    if hasattr(part, 'text')
                ])
            
            session_id = context.request.params.session_id or context.task_id
            logger.info(f"ğŸ“ ç”¨æˆ·è¯·æ±‚: {user_message}")
            logger.info(f"ğŸ”– ä¼šè¯ID: {session_id}")
            
            # ğŸŒŸ å…³é”®ï¼šè¿™é‡Œä½¿ç”¨yieldæµå¼å¤„ç†å™¨
            # æ¯æ¬¡yieldéƒ½ä¼šå®æ—¶å‘é€çŠ¶æ€ç»™å®¢æˆ·ç«¯
            async for update in self.streaming_processor.process_resources_stream(user_message, session_id):
                
                # æ ¹æ®æ›´æ–°ç±»å‹å†³å®šå¦‚ä½•å‘é€ç»™å®¢æˆ·ç«¯
                await self._handle_stream_update(update, event_queue)
                
                # å¯é€‰ï¼šè®°å½•è¯¦ç»†æ—¥å¿—
                logger.info(f"æµå¼æ›´æ–°: {update['type']} - {update['message']} ({update.get('progress', 'N/A')}%)")
            
            logger.info(f"âœ… ç¼–æ’ä»»åŠ¡å®Œæˆ: {context.task_id}")
            
        except Exception as e:
            logger.error(f"âŒ ç¼–æ’ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            
            # å‘é€é”™è¯¯æ¶ˆæ¯
            await event_queue.enqueue_event(
                new_agent_text_message(f"âŒ ç¼–æ’å™¨æ‰§è¡Œå¤±è´¥: {str(e)}")
            )
            
            # å‘é€é”™è¯¯è¯¦æƒ…
            error_result = {
                "status": "error",
                "error": str(e),
                "error_type": type(e).__name__,
                "task_id": context.task_id
            }
            
            await event_queue.enqueue_event(
                new_data_artifact(
                    data=error_result,
                    artifact_type="application/json",
                    title="error_details"
                )
            )
    
    async def _handle_stream_update(self, update: dict, event_queue: EventQueue) -> None:
        """
        å¤„ç†æµå¼æ›´æ–° - å°†yieldçš„ç»“æœè½¬æ¢ä¸ºå®¢æˆ·ç«¯æ¶ˆæ¯
        è¿™å±•ç¤ºäº†yieldåœ¨Agentå†…éƒ¨çš„å®é™…åº”ç”¨
        """
        
        update_type = update.get("type")
        message = update.get("message", "")
        progress = update.get("progress", 0)
        data = update.get("data", {})
        
        # 1. å‘é€è¿›åº¦æ¶ˆæ¯
        if update_type in [
            "workflow_start", 
            "step_start", 
            "step_complete", 
            "workflow_complete",
            "workflow_error"
        ]:
            # é‡è¦é‡Œç¨‹ç¢‘ - æ€»æ˜¯å‘é€
            progress_text = f"[{progress}%] {message}" if progress >= 0 else message
            await event_queue.enqueue_event(
                new_agent_text_message(progress_text)
            )
            
        elif update_type in [
            "sub_agent_progress", 
            "batch_progress", 
            "file_operation"
        ]:
            # è¯¦ç»†è¿›åº¦ - é€‰æ‹©æ€§å‘é€ï¼ˆé¿å…æ¶ˆæ¯è¿‡å¤šï¼‰
            if progress > 0 and progress % 20 == 0:  # æ¯20%å‘é€ä¸€æ¬¡
                progress_text = f"[{progress}%] {message}"
                await event_queue.enqueue_event(
                    new_agent_text_message(progress_text)
                )
        
        # 2. å‘é€ç»“æ„åŒ–æ•°æ®
        if update_type in ["step_complete", "workflow_complete"] and data:
            # å‘é€JSONç»“æœæ•°æ®
            try:
                await event_queue.enqueue_event(
                    new_data_artifact(
                        data=data,
                        artifact_type="application/json",
                        title=f"{update.get('step_name', 'result')}_data"
                    )
                )
            except Exception as e:
                logger.warning(f"âš ï¸ å‘é€æ•°æ®å¤±è´¥: {e}")
        
        # 3. ç‰¹æ®Šå¤„ç†
        if update_type == "workflow_complete":
            # å·¥ä½œæµå®Œæˆ - å‘é€æœ€ç»ˆæ€»ç»“
            summary = data.get("summary", {})
            if summary:
                summary_text = f"""
ğŸ¯ **æœ€ç»ˆå¤„ç†ç»“æœ:**
â€¢ ğŸ“ åˆ†ææ–‡ä»¶: {summary.get('total_files_analyzed', 0)} ä¸ª
â€¢ ğŸ¨ å¤„ç†å›¾ç‰‡: {summary.get('images_processed', 0)} ä¸ª  
â€¢ ğŸ”„ æ›¿æ¢æ–‡ä»¶: {summary.get('files_replaced', 0)} ä¸ª
â€¢ â±ï¸ å·¥ä½œæµID: {data.get('workflow_id', 'N/A')}
â€¢ ğŸ• å¤„ç†æ—¶é—´: {data.get('duration', 0):.2f} ç§’
                """.strip()
                
                await event_queue.enqueue_event(
                    new_agent_text_message(summary_text)
                )
        
        elif update_type == "workflow_error":
            # é”™è¯¯å¤„ç† - å‘é€é”™è¯¯è¯¦æƒ…
            if data:
                await event_queue.enqueue_event(
                    new_data_artifact(
                        data=data,
                        artifact_type="application/json", 
                        title="error_details"
                    )
                )
    
    async def cancel(
        self, 
        context: RequestContext, 
        event_queue: EventQueue
    ) -> None:
        """å–æ¶ˆç¼–æ’ä»»åŠ¡"""
        logger.info(f"ğŸ›‘ å–æ¶ˆç¼–æ’ä»»åŠ¡: {context.task_id}")
        await event_queue.enqueue_event(
            new_agent_text_message("ğŸ›‘ ç¼–æ’ä»»åŠ¡å·²å–æ¶ˆ")
        )


# ç®€åŒ–çš„å•ä¸€åŠŸèƒ½Agentç¤ºä¾‹ï¼Œå±•ç¤ºyieldçš„åŸºæœ¬ä½¿ç”¨
class SimpleStreamingAgent:
    """
    ç®€å•çš„æµå¼Agentç¤ºä¾‹
    å±•ç¤ºyieldåœ¨å•ä¸€Agentä¸­çš„åŸºæœ¬ä½¿ç”¨æ¨¡å¼
    """
    
    async def process_files(self, file_paths: list) -> dict:
        """
        å¤„ç†æ–‡ä»¶åˆ—è¡¨ - ä½¿ç”¨yieldæä¾›å®æ—¶è¿›åº¦
        è¿™æ˜¯Agentå†…éƒ¨ä½¿ç”¨yieldçš„æœ€åŸºæœ¬æ¨¡å¼
        """
        
        total_files = len(file_paths)
        processed_files = []
        
        for i, file_path in enumerate(file_paths):
            # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†
            await asyncio.sleep(0.5)
            
            # ğŸŒŸ å…³é”®ï¼šyieldå½“å‰è¿›åº¦
            progress = int((i + 1) / total_files * 100)
            
            yield {
                "type": "file_progress",
                "message": f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}",
                "progress": progress,
                "current_file": file_path,
                "completed": i + 1,
                "total": total_files
            }
            
            # å¤„ç†å®Œæˆ
            processed_files.append({
                "file_path": file_path,
                "status": "success",
                "size": len(file_path) * 100  # æ¨¡æ‹Ÿæ–‡ä»¶å¤§å°
            })
        
        # æœ€ç»ˆç»“æœ
        yield {
            "type": "completion",
            "message": "æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ",
            "progress": 100,
            "result": {
                "total_processed": len(processed_files),
                "files": processed_files
            }
        }


class SimpleStreamingAgentExecutor(AgentExecutor):
    """
    ç®€å•çš„æµå¼AgentExecutorç¤ºä¾‹
    å±•ç¤ºyieldçš„æœ€åŸºæœ¬é›†æˆæ–¹å¼
    """
    
    def __init__(self):
        self.agent = SimpleStreamingAgent()
    
    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        """æ‰§è¡Œç®€å•çš„æµå¼å¤„ç†"""
        
        # æ¨¡æ‹Ÿæ–‡ä»¶åˆ—è¡¨
        file_paths = [
            "image1.png", "image2.jpg", "document.pdf", 
            "video.mp4", "audio.mp3"
        ]
        
        # ğŸŒŸ ä½¿ç”¨Agentçš„yieldæ–¹æ³•
        async for update in self.agent.process_files(file_paths):
            
            # å‘é€è¿›åº¦æ¶ˆæ¯
            message_text = f"[{update['progress']}%] {update['message']}"
            await event_queue.enqueue_event(
                new_agent_text_message(message_text)
            )
            
            # å®Œæˆæ—¶å‘é€ç»“æœæ•°æ®
            if update["type"] == "completion":
                await event_queue.enqueue_event(
                    new_data_artifact(
                        data=update["result"],
                        artifact_type="application/json",
                        title="processing_result"
                    )
                )
    
    async def cancel(
        self, 
        context: RequestContext, 
        event_queue: EventQueue
    ) -> None:
        """å–æ¶ˆå¤„ç†"""
        await event_queue.enqueue_event(
            new_agent_text_message("ğŸ›‘ å¤„ç†å·²å–æ¶ˆ")
        ) 