"""
Streaming Resource Processor - ä½¿ç”¨yieldå®ç°æµå¼å¤„ç†çš„Agentæ ¸å¿ƒé€»è¾‘
å±•ç¤ºåœ¨Agentå†…éƒ¨å¦‚ä½•æ­£ç¡®ä½¿ç”¨yield
"""

import asyncio
import logging
import uuid
from typing import Dict, Any, Optional, AsyncGenerator
from datetime import datetime
import os
import httpx

from a2a.client import A2ACardResolver, A2AClient
from a2a.utils import new_agent_text_message, new_data_artifact
from a2a.types import MessageSendParams, SendMessageRequest

logger = logging.getLogger(__name__)


class StreamingResourceProcessorOrchestrator:
    """æµå¼èµ„æºå¤„ç†ç¼–æ’å™¨ - åœ¨Agentå†…éƒ¨ä½¿ç”¨yield"""
    
    def __init__(self):
        # Agentæ³¨å†Œè¡¨
        self.agent_registry = {
            "resource_analyzer": os.getenv("RESOURCE_ANALYZER_URL", "http://localhost:8001"),
            "image_processor": os.getenv("IMAGE_PROCESSOR_URL", "http://localhost:8002"),
            "resource_replacer": os.getenv("RESOURCE_REPLACER_URL", "http://localhost:8003")
        }
        
        logger.info("ğŸ­ StreamingResourceProcessorOrchestrator åˆå§‹åŒ–å®Œæˆ")
    
    async def process_resources_stream(self, user_message: str, session_id: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†èµ„æºçš„æ ¸å¿ƒæ–¹æ³• - è¿™æ˜¯Agentå†…éƒ¨ä½¿ç”¨yieldçš„æ­£ç¡®æ–¹å¼
        æ¯ä¸ªå¤„ç†æ­¥éª¤éƒ½é€šè¿‡yieldè¿”å›çŠ¶æ€ç»™å®¢æˆ·ç«¯
        """
        
        workflow_id = str(uuid.uuid4())
        start_time = datetime.now()
        
        # æ­¥éª¤0: åˆå§‹åŒ–
        yield {
            "type": "workflow_start",
            "workflow_id": workflow_id,
            "message": "ğŸ­ ç¼–æ’å™¨å·²å¯åŠ¨ï¼Œå¼€å§‹åè°ƒèµ„æºå¤„ç†å·¥ä½œæµ",
            "progress": 0,
            "timestamp": start_time.isoformat(),
            "data": {
                "user_message": user_message,
                "session_id": session_id,
                "available_agents": list(self.agent_registry.keys())
            }
        }
        
        try:
            # æ­¥éª¤1: èµ„æºåˆ†æ (0-30%)
            yield {
                "type": "step_start",
                "step_name": "resource_analysis",
                "message": "ğŸ“Š æ­¥éª¤1/3: å¼€å§‹åˆ†æèµ„æºç›®å½•",
                "progress": 10,
                "timestamp": datetime.now().isoformat(),
                "data": {"target_agent": "resource_analyzer"}
            }
            
            # è°ƒç”¨èµ„æºåˆ†æAgent - è¿™é‡Œä¹Ÿä¼šæœ‰å­è¿›åº¦
            async for sub_progress in self._call_agent_stream("resource_analyzer", user_message, session_id):
                # å°†å­Agentçš„è¿›åº¦è½¬æ¢ä¸ºæ•´ä½“è¿›åº¦ (10-30%)
                overall_progress = 10 + (sub_progress.get("progress", 0) * 0.2)
                
                yield {
                    "type": "sub_agent_progress",
                    "step_name": "resource_analysis", 
                    "agent_name": "resource_analyzer",
                    "message": f"ğŸ“Š èµ„æºåˆ†æ: {sub_progress.get('message', '')}",
                    "progress": int(overall_progress),
                    "timestamp": datetime.now().isoformat(),
                    "data": sub_progress.get("data", {})
                }
            
            # åˆ†æå®Œæˆ
            analysis_result = {"files_found": 15, "images": 7, "directories": 2}  # æ¨¡æ‹Ÿç»“æœ
            
            yield {
                "type": "step_complete",
                "step_name": "resource_analysis",
                "message": "âœ… èµ„æºåˆ†æå®Œæˆ",
                "progress": 30,
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "analysis_result": analysis_result,
                    "next_step": "image_processing"
                }
            }
            
            # æ­¥éª¤2: å›¾åƒå¤„ç† (30-70%)  
            yield {
                "type": "step_start",
                "step_name": "image_processing",
                "message": "ğŸ¨ æ­¥éª¤2/3: å¼€å§‹å¤„ç†å›¾åƒæ–‡ä»¶",
                "progress": 35,
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "target_agent": "image_processor",
                    "images_to_process": analysis_result["images"]
                }
            }
            
            # å›¾åƒå¤„ç† - æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
            for i in range(analysis_result["images"]):
                await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                
                image_progress = 35 + ((i + 1) / analysis_result["images"] * 35)
                
                yield {
                    "type": "batch_progress",
                    "step_name": "image_processing",
                    "message": f"ğŸ–¼ï¸ å¤„ç†å›¾ç‰‡ {i+1}/{analysis_result['images']}: image_{i+1}.png",
                    "progress": int(image_progress),
                    "timestamp": datetime.now().isoformat(),
                    "data": {
                        "current_image": f"image_{i+1}.png",
                        "completed": i + 1,
                        "total": analysis_result["images"],
                        "batch_completion": (i + 1) / analysis_result["images"] * 100
                    }
                }
            
            processing_result = {"processed_images": analysis_result["images"], "success_rate": 100}
            
            yield {
                "type": "step_complete", 
                "step_name": "image_processing",
                "message": "âœ… å›¾åƒå¤„ç†å®Œæˆ",
                "progress": 70,
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "processing_result": processing_result,
                    "next_step": "resource_replacement"
                }
            }
            
            # æ­¥éª¤3: èµ„æºæ›¿æ¢ (70-100%)
            yield {
                "type": "step_start",
                "step_name": "resource_replacement",
                "message": "ğŸ“ æ­¥éª¤3/3: å¼€å§‹æ›¿æ¢åŸå§‹æ–‡ä»¶",
                "progress": 75,
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "target_agent": "resource_replacer",
                    "files_to_replace": processing_result["processed_images"],
                    "backup_enabled": True
                }
            }
            
            # èµ„æºæ›¿æ¢è¿›åº¦
            for i in range(processing_result["processed_images"]):
                await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿæ›¿æ¢æ—¶é—´
                
                replace_progress = 75 + ((i + 1) / processing_result["processed_images"] * 20)
                
                yield {
                    "type": "file_operation",
                    "step_name": "resource_replacement",
                    "message": f"ğŸ”„ æ›¿æ¢æ–‡ä»¶ {i+1}/{processing_result['processed_images']}: å¤‡ä»½å¹¶æ›¿æ¢",
                    "progress": int(replace_progress),
                    "timestamp": datetime.now().isoformat(),
                    "data": {
                        "file_name": f"image_{i+1}.png",
                        "operation": "backup_and_replace",
                        "backup_path": f"backup/image_{i+1}.png.bak",
                        "completed": i + 1,
                        "total": processing_result["processed_images"]
                    }
                }
            
            replacement_result = {"replaced_files": processing_result["processed_images"], "backups_created": processing_result["processed_images"]}
            
            yield {
                "type": "step_complete",
                "step_name": "resource_replacement", 
                "message": "âœ… èµ„æºæ›¿æ¢å®Œæˆ",
                "progress": 95,
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "replacement_result": replacement_result
                }
            }
            
            # å·¥ä½œæµå®Œæˆ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            final_result = {
                "workflow_id": workflow_id,
                "status": "success",
                "duration": duration,
                "analysis_result": analysis_result,
                "processing_result": processing_result,
                "replacement_result": replacement_result,
                "summary": {
                    "total_files_analyzed": analysis_result["files_found"],
                    "images_processed": processing_result["processed_images"],
                    "files_replaced": replacement_result["replaced_files"]
                }
            }
            
            yield {
                "type": "workflow_complete",
                "step_name": "completed",
                "message": "ğŸ‰ èµ„æºå¤„ç†å·¥ä½œæµå®Œæˆï¼",
                "progress": 100,
                "timestamp": end_time.isoformat(),
                "data": final_result
            }
            
        except Exception as e:
            error_time = datetime.now()
            logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            
            yield {
                "type": "workflow_error",
                "step_name": "error",
                "message": f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}",
                "progress": -1,
                "timestamp": error_time.isoformat(),
                "data": {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "workflow_id": workflow_id
                }
            }
    
    async def _call_agent_stream(self, agent_name: str, message: str, session_id: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è°ƒç”¨å­Agentå¹¶è¿”å›æµå¼è¿›åº¦
        è¿™å±•ç¤ºäº†Agenté—´é€šä¿¡ä¸­çš„yieldä½¿ç”¨
        """
        
        if agent_name not in self.agent_registry:
            yield {
                "status": "error",
                "message": f"Agent {agent_name} æœªæ³¨å†Œ",
                "progress": 0,
                "data": {"error": f"Agent {agent_name} not found"}
            }
            return
        
        agent_url = self.agent_registry[agent_name]
        
        # è¿æ¥Agent
        yield {
            "status": "connecting",
            "message": f"æ­£åœ¨è¿æ¥åˆ° {agent_name}...",
            "progress": 10,
            "data": {"agent_url": agent_url}
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                # è·å–Agentå¡ç‰‡
                yield {
                    "status": "resolving",
                    "message": f"æ­£åœ¨è·å– {agent_name} çš„Agentå¡ç‰‡...",
                    "progress": 30,
                    "data": {"action": "get_agent_card"}
                }
                
                resolver = A2ACardResolver(
                    httpx_client=client,
                    base_url=agent_url
                )
                
                try:
                    agent_card = await resolver.get_agent_card()
                    
                    yield {
                        "status": "connected",
                        "message": f"å·²è¿æ¥åˆ° {agent_name}: {agent_card.name}",
                        "progress": 50,
                        "data": {
                            "agent_name": agent_card.name,
                            "agent_description": agent_card.description
                        }
                    }
                    
                except Exception as e:
                    yield {
                        "status": "connection_failed", 
                        "message": f"è¿æ¥ {agent_name} å¤±è´¥: {str(e)}",
                        "progress": 0,
                        "data": {"error": str(e)}
                    }
                    return
                
                # å‘é€æ¶ˆæ¯
                yield {
                    "status": "sending_request",
                    "message": f"æ­£åœ¨å‘ {agent_name} å‘é€è¯·æ±‚...",
                    "progress": 70,
                    "data": {"message_length": len(message)}
                }
                
                # æ¨¡æ‹ŸAgentå¤„ç†æ—¶é—´
                await asyncio.sleep(1)
                
                yield {
                    "status": "processing",
                    "message": f"{agent_name} æ­£åœ¨å¤„ç†è¯·æ±‚...",
                    "progress": 90,
                    "data": {"agent": agent_name}
                }
                
                await asyncio.sleep(1)
                
                # æ¨¡æ‹Ÿå“åº”
                mock_response = {
                    "agent": agent_name,
                    "result": "success",
                    "data": f"Processed by {agent_name}"
                }
                
                yield {
                    "status": "completed",
                    "message": f"âœ… {agent_name} å¤„ç†å®Œæˆ",
                    "progress": 100,
                    "data": {
                        "response": mock_response,
                        "agent": agent_name
                    }
                }
                
        except Exception as e:
            yield {
                "status": "error",
                "message": f"è°ƒç”¨ {agent_name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "progress": 0,
                "data": {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "agent": agent_name
                }
            }


# è¿™æ˜¯å¦‚ä½•åœ¨AgentExecutorä¸­ä½¿ç”¨ä¸Šé¢çš„æµå¼å¤„ç†å™¨
class StreamingAgentExecutor:
    """å±•ç¤ºå¦‚ä½•åœ¨AgentExecutorä¸­æ­£ç¡®ä½¿ç”¨yield"""
    
    def __init__(self):
        self.processor = StreamingResourceProcessorOrchestrator()
    
    async def execute_with_streaming(self, user_message: str, session_id: str, event_queue):
        """
        åœ¨AgentExecutorä¸­ä½¿ç”¨yieldå¤„ç†å™¨çš„æ­£ç¡®æ–¹å¼
        è¿™é‡Œæ¥æ”¶yieldçš„ç»“æœå¹¶é€šè¿‡event_queueå‘é€ç»™å®¢æˆ·ç«¯
        """
        
        async for update in self.processor.process_resources_stream(user_message, session_id):
            # å°†yieldçš„æ¯ä¸ªæ›´æ–°è½¬æ¢ä¸ºAgentæ¶ˆæ¯å‘é€ç»™å®¢æˆ·ç«¯
            
            if update["type"] in ["workflow_start", "step_start", "step_complete", "workflow_complete"]:
                # é‡è¦çš„é‡Œç¨‹ç¢‘æ¶ˆæ¯
                message_text = f"[{update['progress']}%] {update['message']}"
                await event_queue.enqueue_event(
                    new_agent_text_message(message_text)
                )
            
            elif update["type"] in ["sub_agent_progress", "batch_progress", "file_operation"]:
                # è¯¦ç»†è¿›åº¦æ¶ˆæ¯ï¼ˆå¯é€‰æ‹©æ€§å‘é€ï¼Œé¿å…æ¶ˆæ¯è¿‡å¤šï¼‰
                if update["progress"] % 10 == 0:  # æ¯10%å‘é€ä¸€æ¬¡
                    message_text = f"[{update['progress']}%] {update['message']}"
                    await event_queue.enqueue_event(
                        new_agent_text_message(message_text)
                    )
            
            elif update["type"] == "workflow_error":
                # é”™è¯¯æ¶ˆæ¯
                await event_queue.enqueue_event(
                    new_agent_text_message(update['message'])
                )
            
            # å¯ä»¥é€‰æ‹©åœ¨æŸäº›å…³é”®ç‚¹å‘é€JSONæ•°æ®
            if update["type"] in ["workflow_complete", "step_complete"]:
                await event_queue.enqueue_event(
                    new_data_artifact(
                        data=update["data"],
                        artifact_type="application/json",
                        title=f"{update['step_name']}_result"
                    )
                )
            
            # åœ¨çœŸå®ç³»ç»Ÿä¸­ï¼Œä½ ä¹Ÿå¯ä»¥è®°å½•æ—¥å¿—ã€æ›´æ–°æ•°æ®åº“ç­‰
            logger.info(f"å·¥ä½œæµæ›´æ–°: {update['type']} - {update['message']} ({update['progress']}%)") 