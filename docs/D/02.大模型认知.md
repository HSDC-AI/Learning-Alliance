Date 2025.5.23
### ：
### 发展史：
[人工智能发展史：1900~2023](https://zhuanlan.zhihu.com/p/703419161)

### 趋势和挑战
趋势：规模扩大、效率提升、定制化专用化、多模态增强、
挑战：伦理安全、数据治理、技术普及、人机协作

## 相关

### GPT（Generative Pre-trained Transformer）：
生成式预训练变换器

**Generative：** 生成式，指模型根据输入内容自动生成新的内容,比如：文章和回答问题等

**Pre-trained：** 预训练，指模型正在使用前，先用海量数据进行学习，掌握语言规律和知识，在用于具体任务（比如上学考试）

**Transformer：** 神经网络架构，依赖于自注意力机制(Self-Attention)的深度学习模型结构，专门用来高效处理语言等序列数据

**Self-Attention：** 自注意力机制，关注句子中所有词之间的关系，从这些关系中提取重要信息。

### 大模型训练的三个阶段：
1. Pre-trained（Pre-trained，预训练）：先用大量数据让模型学会基本知识和语言规律。
2. SFT（Supervised Fine-Tuning，监督微调）：用人工标注的好例子，进一步教模型怎么做得更好。
3. RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）：通过人类的评价和反馈，让模型不断调整，变得更聪明。

### 大模型如何生成内容：
**猜**，其实就是根据前面的内容，预测下一个最有可能出现的词，一步步连起来，最终生成完整的句子或文章。

经典论文：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
### 大模型的工作原理
大模型通过分析输入内容，利用已学到的知识和模式，预测并生成最合适的输出。

比如：输入"今天天气真"，模型会一步步预测下一个最可能的词（token），比如"好"，于是生成"今天天气真好"；再继续预测下一个词，直到生成完整的句子。这就像在玩"你说一句我接一句"的游戏，每次都根据前面的内容猜下一个词。

**Token是什么** 
Token（令牌）是大模型处理文本时的最小单位，可以是一个字、一个词，甚至是一个符号。

比如：
- 在中文里，"今天天气真好"可以被分成5个token：今、天、天、气、真、好。
- 在英文里，"Learning" 可能会被分成 "Learn" 和 "ing" 两个token。

模型会把输入的句子拆分成一串token，然后逐个进行分析和生成。

### :

### :

### :

### :

### :

### :

### :

### :

### RAG(Retrieval-Augmented Generation，检索增强生成):
是一种结合外部知识检索与生成式AI的技术，能先检索相关信息，再生成更准确的回答。